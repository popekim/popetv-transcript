예 안녕하세요 포프입니다
초반편에 이어서 이번편도
포프TV 서치엔진
얘기를 좀 더 할거에요
처음편은 그냥
기능적인 내용
왜 만들었는지 어떤 이코시스템이 있는지
어떤 아이디어를 더
내갖고 더 갈 방향이 어딘지
이런거를 얘기했다면
이번편은 정말
처음 기획이 나오고 나서
처음 생각을 하고 나서 제가 이틀동안
이걸 어떻게 만들었는지를
설명을 하려고 해요
그냥 제가 생각이 흘러가던
방식을
제가 현재 기억하는 한도에서
3-4일 지났기 때문에
그렇게 얘기를 할게요
처음에 이걸 만들기로 했을 때
아주 간단했어요 그냥
구글처럼 검색엔진 있고
키보드 치고 검색 누르면
비디오들 나오게 하자
시작은 그거였죠
간단하게 웹사이트 올리는데 제일 편한게 뭐냐
그러면 닷넷코어 쓰면 되겠구나
그래서 닷넷코어로
빈
웹사이트 만들어갖고
딱 올리는거
그것까지 뭐 한 5분이면 되니까
그거 올렸어요
올리면서
내가 과연 여기에서
유저관리가 필요할까
보통사람의 유저관리는 필요가 없잖아요
그러나
이제
관리자의 기능들이 몇개가 있기 때문에
그 관리자 기능을 하기 위해서
유저관리를 해야겠다
그럼 유저가입이 필요하겠구나
그래서 유저가입을 만드는데
제가 따로 막 유저 아이디 만들고 패스워드 만들면은요



만들고 이렇게 하기가 싫은 거예요 그래서 아 그러면 그냥 우리 흔히
다 쓰고 있는 오어스 제공하는 사람들 있잖아요 서비스들 그럼
오어스 중에 이 서비스를 이용해서 오어스가 되게 하자 단 로그인은
보통 오어스라면 회원가입에 오어스 를 적용하면 아무나 될 수 있게
하잖아요 그 대신에 우리가 이미 알고 있는 그런 내가 알고 있는
사람들만 관리자 권한을 주려면 그냥 앱 세팅 같은데 거기에 이메일
주소를 몇 개 집어넣고 컴마로 분리 시켜서 이 사람들 중에 하나면
이제 가입도 허용하고 로그인도 허용하지만 아니면 그냥 아예 안
되게 그래서 그런 이제 오어스 믿을 모듈을 놓고 집어넣었죠 그럼
일단은 관리자 페이지 보이는 것 까지는 거의가 된 거예요 그럼 그
다음에 이제 제가 하려고 했던 거는 최소한 페이지는 두 개가 있어야
되니까 그럼 메인 페이지는 두 개 가 있어야 되니까 그럼 메인 페이지
가 있고 메인 페이지에서 폼 집어넣고 버튼 누르면 search 페이지로
가는데 이 텍스트로 집어넣은 게 이제 url에 나오게 해야 되니까 그럼
get request 뭐 이런 식으로 해서 redirect가 아니구나 그 누르면 get request에서
뭐 search question mark query 그리고 뭐로 나오게 검색어 들어가는 것
까지 만들자 그래서 그럼 그냥 mbc 페이지로 쉽게 만들 수 있잖아요
그래서 만들고 이제 그것 따라 이제 페이지
내비게이션을 만들고 이제 페이지 내비게이션을 만들고 이제 페이지
내비게이션을 만든 거예요 페이지 내비게이션을 그럼 그 다음
해야 될 일은 당연히 데이터베이스 디자인이었어요 제가 데이터베이스
에 어떤 데이터를 넣어 놓고 뭐 이 데이터를 어떻게 긁을 건지 기본
적으로는 뭐 되게 간단하게 얘기 하면은 뭐 다른 테이블도 있지만
결국엔 제가 필요한 거는 비디오 정보만이 필요했던 거예요 근데
이제 비디오마다 id가 있고 그 id 는 유튜브에서 이미 비디오 id가
한 8글자 9글자 되는 텍스트가 있거든 요 그럼 그걸 id로 만들고 그래서
제 db에는 그게 id로 들어가 있어요 그리고 이제 제가 실제 검색할 때
필요한 정보들 title 뭐 유튜브에서 갖고 와서 긁으면 되는 거고 그리고
description 긁어왔고 그 다음에 이제 필요했던 게 태그였는데 태그는
여러 개거든요 그럼 뭐 이제 당연히 태그 페이지를 따로 아니 테이블
을 따로 만들어서 포린키로 엮어 왔고 여기에 들어간 태그는 이겁니다
라고 보여줄 수도 있는데 제가 그렇게 태그를 따로 검색을 해야지
될 일이 그니까 이제 하나하나 태그 태그를 따로 검색해야 될 일이
생각보다 없었다고 하죠 아니면 다른 식으로 설치를 만들었다고
하고 그래서 그냥 태그를 컴마로 분리시켜서 컬럼 하나에 집어넣
써요 이제 그 다음에 나오는 게 이제 캡션이었거든요 문제는 캡션
이 아까 뭐 전 비디오에서 말했듯이 이제 세 가지가 있어요 책에서 나온
거 아니면 누군가 손수 만들어서 유튜브에 공언해 준 거 아니면
유튜브에서 자동으로 만든 거 그럼 일단 그 셋 중에
가장 좋은 거 하나만 전 쓸 거기 때문에 어차피 그냥 방식을 그런
식으로 만들었죠 지금 캡션이 들어가고 컬럼 하나에 들어가고 이 캡션
타입이 뭔지만 알면은 다른 새로운 캡션이 나왔을 때 이걸 받아와야
말지를 결정할 수 있으니까 그래서 캡션 타입의 세 가지 중에 하나를
놓고요 이름은 정확히 뭐라고 했는지 기억 안 나지만 뭐 커스텀 매뉴얼
스탠다드 뭐 이런 식일 수도 있고 오토 매뉴얼 커스텀 이런 식일 수도
있고 그래서 그걸 놓았어요 그리고 마지막에 있는 거는 그냥 단순하게
뭐 이거 비디오가 뭐 이런 식으로 만들어진 것 같은 그런 식으로 만들어진
것 같은 그런 식으로 만들어진 고객의 Guanso가 실제 출시된 날짜가
언제냐 그리고 최종적으로 우리 db에서 고쳐인 날짜가 언제냐 이
정도만 박아놨어요 그래서 테이블은 그렇게 디자인을 했고 그리고 무슨
제 페이스북에서 누구 한 분이 SQL injection 뭐 되던데 뭐 이런 식으로
얘기했었는데 제가 좀 더 자세히 SQL injection을 달라고 그랬어요 정확히
제가 ORM을 그냥 썼기 때문에 SQL injection에 대비가 돼 있다고 생각을
했는데 이제 그분 말로는 인젝션 한 게 실현이 되더라라는 식으로
말씀을 하시는데 정확히 뭘 실어 내는 걸 알 butt CAM 근데 왜 번역이
실행하셨는지 모르기 때문에 제가 좀 퀄이는 달라고 그랬죠
DB도 그렇게 만들었어요
그러면 그 다음에 할 게 가장 큰 일이었는데
그 유튜브 오어스를 연결을 해서
거기서 이제
데이터를 긁어오는 거였죠
그러면 이제 뭐 유튜브 APL 막 뒤져보고 뭐 하면서
뭐 제가 필요했던 거는 결과적으로는
그 오프라인 모드가 필요했던 거예요
이 서버에서 이 토큰을 가지고 있고 계속
하루에 한 번씩 몇 시간에 한 번씩 유튜브 API 때려서
새로운 정보가 있는지 보고 가져오고 아니면은
그 관리자가 이제 관리자 페이지 가서 버튼을 누르면
그거에 따라 유튜브 좀 작업 좀 진행하고
이런 거였는데
그래서 오프라인 모드 오어스 문서 뒤져가면서
뒤지고 여기서 만들었죠
거기서 뭐 베스트 뭐
프랙티스 이거 이렇게 해야 돼 라고 써있어서
따라 했다가
실제 리프레쉬 토큰 안 날라와서 고생도 하고
뭐 어쨌든 다시 한 번 구글의 좀 문서화 능력이 약간의 좀
좀 안타까움을 느끼는 부분도 있었지만
대부분 괜찮았고
그리고 이제 유튜브 API 페이지 가면은 제가 실행을 해볼 수 있어요
그 웹사이트 안에서 테스트로
그래서 이제 그런 거 해보면서 정보를 막 수집하기 시작을 한 거죠
처음의 개념은
제 이제 채널이 있으니까 채널에 있는 비디오를 다 불러온 다음에
거기에서
제 본방에 들어간 비디오만 골라오는 게 목적이었어요 사실은
근데 그렇게 하려다 보니까
내가 이게 본방인지 아닌지 알 수 있는 방법이 쉽지가 않은 거예요
물론 라이브 한 거는 본방이 아니라고 할 수도 있겠지만
뭐 그 외에 제가 따로 올린 비디오도 좀 있거든요
그러면 이걸 어떻게 디텍트를 해야 될까
그리고 또 하나는 제가 비디오를 올릴 때
막 1화 2화 이런 식으로 이름을 달아서 올린 적은 없어요
근데 제가
제목엔 없지만
처음 비디오를 올린 파일에는
숫자가 달려있어요 1,2,3,4 이런 식으로
그래서 제가 몇 환지를 거기서 알아서 봐갖고
내 설치 엔진에 보여주고 싶다는 생각도 했는데
제가 가끔 들떨어져갖고
똑같은 숫자를 몇 번 집어넣은 적이 있더라구요
그래서 아 그것도 안되는데 그러면 올린 순서대로 찾아야 되는데
아 뭐 그거야 이제 비디오 퍼블리시된 데이트대로 보여주면 되니까 상관이 없지만
이제 그 순간에 이 숫자를 기반으로 해서 본방을 찾아볼까라는 생각도 했지만 좀 건너뛰고

너무 핵 같았고
그래서 고민하다가 어떻게 했냐면은
채널로 시작하는 게 아니라
플레이리스트에서 시작하기로 했어요
제가 다행히도
여태까지 나왔던 본방 모듈을
플레이리스트 하나에 집어넣어서
거기에 이제 본방 전체라는 플레이리스트가 있어요
그러면 어차피 새로 올라오는 본방들도 계속 거기에 올라갈 거고
라이브는 거기에 올릴 생각이 없으니까
그럼 이 플레이리스트가 바뀌는 걸 보고 있다가
이제 이 플레이리스트가 바뀌었을 때마다 새로 정보를 긁어와서
이제 보여주자
라는 식으로 했던 거죠
그래서 처음에는 채널 기반으로 해서 모든 비디오를 긁어오려고 했지만
결과적으론 플레이리스트
내가 원하는 플레이리스트는 이거야 라고 등록을 하나 해두고
그 플레이리스트에서 새로운 비디오가 올라올 때마다
새로운 비디오 정보들을 긁어오게 해놨어요
그리고 뭐 그 안에
모든 유튜브 API에 이 태그가 달려있거든요
이 태그라는 거는 이제 컨텐츠 해쉬라고 보면 맞는데
그거를 따라서 이제
음
새로운 비디오가 올라왔나 안 올라왔나를 검색하는 법도 있었는데
그거는 이제 마지막으로 안 했던 이유가
유튜브 API가 페이지네이션이 돼있고
한 번 API 호출할 때 50개 이상의 정보를 못 불러오고
거기 달린 이 태그는 그 페이지에 관한 건데
제가 비디오를 보여주는 순서가 옛날 것부터 요즘 순서기 때문에
새로운 비디오가 올라갈 때 제일 밑에 들어가요
그럼 처음 페이지 안 나오거든요
그럼 거기서 과연 이 태그가 바뀌는지 안 바뀌는지 정확히 확신이 없어서
그런 걸 안 하고 대신
이 플레이리스트에 있는 비디오 수가 몇 개냐
그리고 내가 저번에 봤을 때 비디오 수가 몇 개였냐에 따라서
비디오 숫자가 바뀌었으면은
새로 정보를 긁어오는 걸로 바꿨어요
그러면 일단 플레이리스트에 긁어올 수 있는 정보들은
아
이제 뭐 여러 가지가 있는데
어떤 정보를 불러오냐에 따라 이제 과금이 좀 달라져요
유튜브 API 쓰는데 물론 처음 얼마간은 프리고
한 달에 얼마 얼마 얼마 정도는 공짜고
그래서 이제 거기서 받아왔을 때 받아왔던 게
최소한 이 플레이리스트에 있는 비디오 아이템들 아이디죠
근데 그게 이제 뭐 아이디만 불러오면 공짜긴 한데
그 아이디가 플레이리스트 안에 있는 아이디이기 때문에
실제 비디오 아이디는 아니었고
거기서 비디오 정보를 좀 더 보여주는 그 스니펫이라고 해요
그걸로 갖고 최소한 정말 비디오 아이디가 뭔지까지 불러왔어요
그럼 그거 긁어와서 아 비디오 아이디 이런 것도 있고
그럼 DB 이제 검색해서 없는 것도 새로 추가해 놓고
그럼 거기까지 긁어오는 거
그 다음 단계는 다시 DB를 내가 봐서
아 비디오 아이디는 있는데
타이틀이라던가 디스크립션 이런 게 없는 것들
그런 것들은 이제 다시 다시 유튜브 API를 이제 때려서
요 비디오에 대한 정보를 불러와
근데 비디오에 대한 정보를 불러오는 게
최소 50개까지 한 번에 넣을 수가 있어요
비디오 아이디 50개 넣고 최대 50개
그거에 대한 정보를 다 불러와서
거기서 이제 퍼블리쉬한 데이트
그리고 캡션 태그
아 캡션 아니야
태그, 타이틀, 디스크립션까지 불러와서 저장을 해요
그 다음에 캡션은 이제 제가 쓴 지금 유튜브 API 중에 가장 비싼 부분인데
캡션은 다른 이제 API 엔드포인트가 있더라고요
그래서 그 캡션을 이제 리스트를 뽑으면서
그 줘야 되는 아이디가 비디오 아이디를 줘야 돼요
이 비디오에 있는 캡션 리스트를 뽑아줘
그럼 캡션 리스트를 뽑아주거든요
그럼 거기서 볼 수 있는 정보들이 이제 여러 가지가 있어요
근데 아까 처음 작업은 플레이스트에서
새로 올라온 비디오 긁어서 DB에 넣기였고
두 번째는 새로 올라온 비디오에 대한
뭐 타이틀, 디스크립션 뭐 이런 거 태그 넣기였고
세 번째 단계로는 다시 또 그 DB를 긁으면서
아직 캡션이 없는 그런 엔트리에 대해서
캡션 API를 이제 하나씩 하나씩 호출해야 돼요
비디오 50개를 호출할 수 있는 게 아니라
호출해서 거기서 어떤 캡션이 있느냐
그러면 둘 중에 하나가 보통 나와요
하나는
둘 다 나올 수도 있고
하나는 자동으로 만들어 준 거
하나는 이제 누군가 공언하고 올린 거
이제 스탠다드 타입이라고 하는데
그러면 이제 보이면은 그 중에서 내가 제일 원하는 캡션을 고르는 거예요
뭐 한국어로 당연히 되어 있고
지금 현재 퍼블리시되어 있는 상태의 캡션이고
그리고 스탠다드면은 그거를 가져오고
아니면은 fallback 로직으로 이제 오토매틱으로 만든 걸 가져오는 거죠
그럼 이제 그 아이디를 갖고 와요
캡션 아이디
그러면 다시 API 한 번 더 호출해서
캡션 다운로드 API가 되는 거예요
이게 제일 비싸
그러면 이제 다운로드 받아서 캡션 딱 받아지면은
온갖 이제 캡션도 종류가 여러 가지가 있어요
그 포맷이 여러 가지가 있는데
종류가 아니라
그 포맷 중에 제가 가장 손팔싱을 하기 쉬운 것처럼 보이는 포맷을 다운받아서
코드를 짠 거죠
이렇게 이렇게 하면 타임 스탬프 빼버리고
뭐 이렇게 뭐
쓸데없이 스페이스 많은 거 좀 없애고 뭐
새 라인으로 되어 있는 거 어차피 한 줄에 바꿔 상관없으니까 그냥
띄어서 한 줄에 바꿔
이런저런 약간 포맷 룰이라던가 그런 클리닝 룰을 넣어갖고
캡션을 다 클리닝해서 그냥 텍스트 큰 거 하나로 만들어서
DB에 일단은 박아요
그래서 일단은 그렇게 해서 데이터 긁어오는 걸 마쳤어요
그럼 처음에 만들 때는 이게 그냥
단계는 네 단계지만
이제 뭐 자동으로 실행되는 뭐 이런 단계를 안 만들었기 때문에
그냥 제가 어드민 패널 하나 만들어서 페이지
MBC 페이지죠
그럼 버튼 하나 누르면 이거 액션 이렇게 호출해갖고
그거
이제 함수 호출하게 만들고
그래서 테스트 해보고 아 되는구나
그리고 이제
그럼 그 다음에는 이제 해야 될 일이
일단은 이거 데이터를 긁어왔으니까
그 다음에는 검색은 가능할 거 아니에요 이제 긁어왔으니까
이제 검색을 만들어 보자
그래서 아까 처음에 말했듯이
쿼리에 넣어갖고 딱 버튼 누르면은 이 페이지가 뜨고
그 쿼리 스트링이 들어오잖아요
그리고 이제 단어가 한 개 이상 들어올 때가 고민이었는데
이거를 두 단어가 동시에 나오는 걸로 처리할 거냐
아니면 따로따로 나온 거를 따로따로 이제 어떤 그 비디오가 가장 이 단어를 많이 갖고 있는지 봐서
이제 그걸 합쳐갖고 점수 제대로 해서 보여줄 거냐 이 고민이었는데
지금은 따로따로 보는 걸로 해요
그러니까 예를 들어서 포프 스페이스 넣고 TV 넣으면은
포프를 검색해요 그냥 모든 비디오에서
그래서 이거 이제 나온 횟수라던가 아니면 이제 뭐 타이틀에서 나오는지 뭐 태그에서 나오는지
그런 거에 따라서 이제 좀 빈도수를 바꿔서
중요도를 바꿔서
타이틀이 좀 더 중요하고 태그가 좀 더 중요하고
캡션은 워낙 이상한 얘기들 많이 하니까 뭐 한 좀 덜 중요하고 이런 식으로 해서
각 비디오마다 각 검색어마다 이제 점수가 몇 점인지 매겨서
그걸 더한 다음에 이제 가장 점수가 많은 걸 우선적으로 보여주는 식으로 대충 만들었어요
모든 거를 다 그냥 처음에 만들 때는 모든 걸 SQL로
그냥 SQL 이제 검색할 수 있잖아요
이 텍스트가 이거를 이 단어를 포함하고 있으면
목록을 줘 뭐 그 정도였어요
그래서 일단 테스트만 돌게 그래서 테스트를 딱 올리고 도니까 돌긴 돌아요
검색어 넣으면은 검색되고 대충 돼요
그럼 거기까지 일단 했고
그럼 그 다음에 제가 생각한 거는
아 이 검색 속도를 빠르게 만들어야 되려면은 당연히 구글이 하듯이 이제 인덱싱을 미리 하는 게 좋겠구나
라는 생각이 있었는데 그건 조금 더 이제 복잡하니까
그 전에 자동으로 유튜브 정보를 다 불러오는 걸 만들자
이거는 뭐
당연히 아키텍스적으로 얘기하면은 뭐 애저 펑션 같은 거 하나 올려서 타임 트리거 박아서 해도 되는데
어차피 되게 간단한 서비스고
그리고 굳이 여러 컴포넌트 만들 필요도 없을 것 같아서
그냥 그 닷넷 코어 이제 웹 서버 안에
그 롱러닝 테스크를 만드는 법이 있어요
예전에 닷넷 프레임워크에서는 되게 쉬웠는데
ASP 닷넷에서는
지금 ASP 닷넷 코어 나온 다음에 약간 좀 그게 쉽지 않았다가
요즘 또 다시 쉬워진 게 쉬워져서
이건 언제나 쉬워져서
이제 한 번 실행되는 백그라운드 워커
그러나 이제 한 번 이 잡을 실행하고 나서
뭐 몇 초 동안 쉬는 거죠
몇 달 동안을 쉬든 몇 시간을 쉬든
딜레이 넣어갖고 이제 아 쉬었다가 다시 깨어나서
작업하는 그런 스레드를 박았고
그 스레드가 하는 일도 되게 단순했어요
왜냐하면은 아까 제가 말했듯이
그 네 단계로 나와갖고 유튜브 비디오 정보를 다 긁어오잖아요
그냥 그 함수들을 차례대로 실행하면 돼요
실행하다가 새로운 거 없으면 그냥 멈춰 이래서 되는 거고
그래서 실제 웹페이지에서 버튼 누르면은 실행되든
어드밋 패널에서 버튼 누르면 실행되던 그 네 단계 로직을
백그라운드 스레드로 똑같이 실행되게 만들어 놨고
뭐 그냥 굳이 둘이 동시에 실행될 이유는 없기 때문에
뭐 큐를 넣어갖고 이제 한 번에 하나씩만 실행되게
생각을 했었죠
왜냐하면은 누가 어드밋 패널에서 버튼 눌러갖고 하는 거나
백그라운드 스레드에서 버튼을 자동으로 대는 거나
이게 같이 일어날 일은 없잖아요
근데 뭐 어차피 이게 스케일이 엄청 될 것도 아니고 당장
그리고 또 큐 집어넣고 새로운 컴포넌트 만들어갖고
뭐 이거 뭐 돈도 더 내기 싫었고
그래서 일단은 그냥 싱글 서버로 보고 있기 때문에 일단은
다른 사람한테 서비스하지 않는 이상은
당장은 그냥 그 서버 위에 있는 그 슬림 세마포어 써갖고
락을 그냥 거는 걸로 했어요
나중에 스케일이 될 땐 당연히 큐 방식으로 가는 게 정상이죠
그래서 일단은 그렇게 했고
그래서 이것도 끝났어요
그럼 이제 스타일링 같은 건 남았는데 뭐 스타일링 이하 뭐
부스트랩 써갖고 하면 되는 거고
그리고 뭐 당연히 주변에 다른 사람을 좀 시켰어요
제가 방향만 좀 잡아두고 이런 건 이렇게 해주고
뭐 페이지네이션 넣어야 되니까 또 페이지네이션을 놓고
이제 지금 설치 엔진 가보신 분들은 알겠지만 페이지네이션 이제 구글하고 되게 비슷하게 해놨죠
구글하면 구~글 써있는 걸 저는 포~포 이렇게 써놓은 게 전부고
이제 마지막 남은 게
그리고 어찌 보면은 가장 기술적으로 이제 귀찮았고 실패도 했던 게
이제 인덱싱 쪽이었어요
인덱싱이라는 게 뭐냐
제가 지금 검색 엔진 당장 만들었던 방식은 그냥 sql에 있는 텍스트를 실시간으로 뒤져갖고 보여주는 방식이에요
지금 비디오 400개 정도밖에 안 되고 캡션 자료 뭐 있어봐야
뭐 어떤 검색어 루턴 간에
검색 시간은 한 3초 정도거든요 많이 걸려봐야
그런데 데이터가 이게 많아지고 많은 사람을 서비스하다 보면 당연히 느려지겠고
그래서 인덱싱을 그냥 넣고 싶었고 예전부터 해보고 싶었기 때문에 약간
그래서 하다가 결국엔 실패해서 옛날 방식으로 가고 있는데
뭐 이 정도 서비스에서는 옛날 방식도 문제가 없다고 보긴 해요
당연히 속도를 빠르게 하는 방법도 있었지만
근데 인덱싱을 할 때 인덱싱이 뭐였냐면은
막 되게 많은 문서가 있잖아요 구글이 하고 있는 거기도 해요
굉장히 많은 문서가 있어요 전 세계에
그럼 문서 하나를 봤을 때 거기서 나오는 단어들이 있어요
중요하지 않은 단어도 있고 중요한 단어들도 있죠
예를 들어서 영어라면은 뭐 전치사 to in 뭐 이런 거 별로 안 중요하고
뭐 정관사 이런 거 별로 안 좋아하고 안 중요하고
그런데 중요한 단어들이 있잖아요 뭐 명사라던가 동사라던가
그러면 그 이렇게 것들을 모아서
이 문서에서 이 단어가 몇 번이 나오는지 등을
이제 표 같은 걸로 만들어 놓는다고 생각하시면 돼요
결과적으로는 그건데 그러면 전 세계에 있는 모든 문서를 다 모으고
그러면 이제 아 요 단어가 가장 많이 나오는 문서는 이거야
그럼 이제 단어 하나로 모든 걸 해결할 수는 없잖아요
그럼 문장이 있고 여러 단어가 있으면 이 단어를 벡터로 줄줄이 세워놓고
이 단어가 이렇게 줄줄이 벡터로 아니면 비슷한 곳에서 나오는 이런 뭐
가장 많이 나오는 문서는 또 이런 거야
이런 식으로 해서 이제 랭킹을 매기는 방식이에요
그러면 검색을 할 때 빠른 이유는
실제 그 문서를 전부 다 검색을 하는 방식이 아니라
이게 아마 구글이 나오기 전에 많은 회사들이 이 짓을 했었겠죠
그래서 검색이 느렸던 거겠고
그런 방식이 아니라 미리 이 문서에서 아 이 문서는 이 단어가 이만큼 중요하고
저 단어가 이렇게 중요해라는 그런 정보만을 뽑아서
그거를 이제 표로 집어넣은 거예요
그리고 그걸 다 합치면은 이제 요 단어가 나오면
그 다음에 중요한 문서만 이 문서 이 문서 이 문서 이 문서 이 문서 순으로 중요해라고
그냥 표를 만들어 놓은 거고 검색이 거기에 들어오면은
찾아갖고 그 문서를 순서대로 보여주면 되는 거예요
그래서 당연히 구글이 이걸로 이제 혁신적으로 2000년대였는지 90년대였는지 나왔던 회사고
그 다음에 이제 굉장히 많은 설치 엔진들이 다 인덱스 방식으로 바뀌었죠
옛날에 윈도우에서 파일 컨텐츠 설치할 때도 인덱스 방식이 아니었어요 한동안
아마 윈도우에서 파일 컨텐츠 설치할 때도 인덱스 방식이 아니었어요 한동안
윈도우 7인가에서 들어왔던 거 같아요
그럼 이제 이거를 어떻게 하냐
당연히 제가 직접 만들진 않죠
제가 알고 있었던 건 뭐냐면 예전에 다른 회사에서 했을 때 루신 엔진을 썼어요
루신 엔진이 그걸 해주는 엔진이고 오픈소스고 아파치 파운데이션 내에서 개발했던 걸로 알고있고
그 성능이 되게 좋아요
그래서 실제 루신 엔진을 하려고 막 뒤지면서
요즘은 이제 설치 엔진이 루신이 아니라 이제
엘라스틱 서치라던가 이런거 많이 쓰잖아요
그래서 엘라스틱 서치 갈까?
라고 하다가 그것도 다른 컴포넌트 올려야되고 서버 따로 돌려야되고
비디오 400개 있는 데서 뭐 이게 왜 필요해
라고 했을 때 안 돌렸는데
그걸 돌렸으면 모든 문제가 해결됐을 것 같긴 한데
일단 그건 뒤에 얘기하기로 하고
찾다 보니까 재밌는게 뭐였냐면은
제가 엘라스틱 서치를 예전에 올린 적도 있고 루신을 쓴 적도 있어요
근데 루신은 그냥 라이브러리로 그냥 돌릴 수 있는거에요 서버에서
C샵 용도 나왔어요
근데 뒤지다보니까
엘라스틱 서치 자체가 루신엔진 기반이더라구요
루신엔진이 원래 한 서버에서 인덱스 빌드하고 이런거였다면
그거를 루신엔진을 갖다가
디스트리뷰티드 서치엔진으로 바꾼게
이제 엘라스틱 서치라고 보면 맞아요
그래서 어차피 같은 로직이고 같은 알고리즘이더라구요
그래서 "아 됐다"
그래서 이제 인덱스 빌드하고 모든 문서를 갖고 와서
이 문서를 바로
여기서는 이런 타이틀과 이런 태그와 이런 디스크리미션과 이런 캡션을 가지고 있어
자, 네 맘대로 인덱스를 빌드해
인덱스를 빌드할 때 네가 써야 되는 무슨 알고리즘은 이거야
라고 골라주는 몇 가지 알고리즘들이 있어요
플러그인 방식이긴 한데
너 써요, 어 빌드 됐어
어이, 400개 했는데
어, 인덱스 파일이 딱 1.9메가밖에 안 나와요
아, 괜찮구나
검색을 했어, 안 나와
그래서 그때부터 열심히 이제 더 파보기 시작했죠
왜 안 될까
근데 이제 보니까
뭐, 결론적으로 얘기하면은
그 인덱스 엔진 자체가
영어를 되게 기준으로 만든 거라고 보면 맞아요
그래서 이제 한국이나 중국어나 일본어로 할 때는
이 알고리즘을 쓰면 된다 이런 식으로 나오는데
그게 별로예요
중국어 쪽은 잘 된다고 제가 얘기를 들었는데
한국말은 정말 별로예요
그래서 검색을 해본 결과 한국 커뮤니티 쪽에서도
이거는 쓰레기다
거의 이런 식으로
그런 느낌이거든요
근데 이유가 뭐냐면
인덱스를 빌드할 때
중요하지 않은 단어들을 빼는 게 맞아요
예를 들어서 영어로 했을 때
뭐, I am a boy 이러면은
I는 별로 필요 없는 얘기고
M도 별로 필요 없는 얘기고
어도 필요 없는 얘기잖아요
그럼 이 세 개 단어를 무시하고
boy만 가지고 이제 인덱스를 만드는 그런 방식이에요
뭐, 전치사라던가
뭐, 이런 모든 게 다 포함이에요
그럼 이제 이것들을 보통
하는 얘기가
스탑워드라고 얘기를 많이 해요
이제 기술적으로 그러면
스탑워드의 목록을 다 집어넣어 놓고
이런 쓸데없는 언어들
이게 나오면 무시하고 다 빼버리고
남은 걸로만 이제 인덱스 검색엔진을 돌려라
이게 정상이에요
근데 이제 영어권에서 이게 쉬운 이유는
모든 전치사, 모든 정관사
이런 게 그냥 단어 하나예요 단어
단어별로 스페이스 다 분리하고
뭐, 뭐, 컴마 뭐, 뭐
이렇게 마침뼈 다 분리한 다음에
딱 영어 단어 나온 거에서
필요 없는 것만 싹싹 골라내면 돼요
근데 한국어는 그게 안 돼요
왜냐면은 뭐, 조사, 전치사 이런 게 그냥
다른 단어에 갖다 붙잖아요
그러면 이거를 정말 제대로 분석을 해서
아, 이거는 조사고, 이거는 전치사고 해서
이걸 따로따로 분리해주는 로직이 나오지 않는 이상은
그걸 실제 전처리를 하지 않는 이상은
이 스탑워드만 가지고 분리할 수 있는 방법이 없어요
근데 제가 이제
이런 거를
만든 다른 알고리즘도
리서치하면서 봤는데
대부분 그냥 하는 방식대로
뭐, 이게 뒤에 붙고 앞에 붙고 이런 생각 없이
그냥 이거 나오면 무시해라는 게 전부였고
실제 그 단어가 중간중간 다른 단어에도 나올 수 있거든요
다른 그 용맥, 아니, 문법상
아, 그니까 문맥상에
근데 그것까지 무시하게 되더라고요
그래서 이 단어분리와 단어 삭제
스탑워드 걸리는 거 제대로 안 돼있고
그리고 루신엔진에서 그 cjk, cjk라고 이제
그게 차이나, 저펜, 코리아 쪽 쓰는 그런
이름이 정확히 인덱서였나?
어쨌든 저는 알고리즘이라고 일단 할게요
그거를 보는데 그거는
비그람이라고 해갖고 이제
뭐
한자 같은 경우에 보면은 이제
한자 단어를 두 개를 이렇게 쓰면은
이거 바꿔갖고 해도 의미가 비슷한 경우가 많나봐요
그래서 그런 거를 이제
같은 걸로 처리해갖고 이제 인덱스 검색을 하는데
한국말은 그게 아니잖아요
한국말은 내가 바보라고 했다고
보바하고 같은 의미는 아니거든요 사실은?
그래서 그거는 전혀 아니더라고요
그래서 더 뒤지다 보니까
그 루신용으로 이제 한국말을 잘 돌게 만들겠다고
이제 그 플러그인을 만드신 분이 있어요
제가 이름은 까먹었지만
그 검색하면 나와요
아리랑 루신 플러그인이라고 하면은
그게 한국 그 아까 말했던
인덱스 검색하는 알고리즘을 만드는
그런
플러그인인데
대충 소스코드는 봤어요
자바로 짜여있고 루신 자체가 원래 시작은 자바거든요
닷넷 버전은 왔긴 한데
제가 닷넷으로 포팅해볼까라는 생각을 하면서
한번 뒤져보기도 했는데
제가 아까 전에 말했던 그런
조사분리라던가 이런 것들 자체가
제가 이제
그냥 상식적으로 어떻게 하면 이
검색어만 뽑아낼까라는 생각을 했을 때
거기에 딱 들어맞는 모습도 아니어서
잠깐 주저했고
차라리 이거를 내가 처음부터
만드는게 더 낫지 않을까라는 생각도 잠깐 했지만
일이 너무 많아지고
그래서
나중에 기회가 되면은
어차피 지금 루신 엔진이 맞고
엘라틱 설치 엔진이 많이 쓰이고
이제 엘라틱 설치 엔진이 많이 쓰이니까
근데 엘라틱 설치 엔진은 닷넷으로 올리는 사람은 없죠
그냥 모듈을 올리는 거니까
아리랑 플러그인을 꼽아서 쓸 수 있고
루신 닷넷은 굉장히 많은 플러그인을
포팅을 해놨지만
이제 아리랑은 아무래도
덜 유명한 거겠죠
한국 사람 사용자도 적은 거 같고
그거 플러그인은 포팅된 게 없더라고요
그래서 나중에 기회가 되면 이걸 좀 더 자세히 읽어보고
닷넷으로 포팅을 해갖고
차라리
딴 사람도 이제 도움받게 하면 좋지 않을까 생각을 했지만
제가 여태까지
코드를 본 것만으로는
정말 제가 생각하는 만큼의 좋은
설치 엔진 인덱스가 나올 것 같지는 않았어요
그래서
그렇게 이렇게
생각을 하다 보니까
아 이건 배보다 배꼽이 크구나
차라리 3초 딜레이 있는 게
나은 거구나 라는 생각을 먹고
이제 루신을 잠깐 접고
코드 다 들어갔던 거 다 빼고
아 네이버 이제 검색 엔진에 대해
굉장히 많은
존경심을 가지게 된 것도 그런 부분이고요
뭐 네이버가 무슨 요즘
욕을 많이 먹고 있는 것 같긴 한데
그냥 이
쉽지 않은 언어를
검색이 되게 만들었다는 거
그래서 그리고 실제
구글에 비해 검색을 이제
문장으로 쫙 하면은 잘 안 되는 것도 되게 많아요
생각보다 검색 결과도 안 좋은 것도
많은 것도 사실이고 영어에 비해
근데 그래도 그 정도라도
만들어서 그 정도 속도로 만들었다는 거 자체가
아 얘네는 원천 기술이 확실히
있는 애들이구나 저거 할
려고 해도 못하는 애들이
좀 많긴 많겠구나라는 생각을 하긴 했어요
그리고 이제
제가 루신을 아까 또 포팅을 하네
얘기를 하다가 또 주저하고 있는 게
뭐냐면 제가 이
3초 딜레이가 이제
문제가 될 정도로 이제 문서는
많아질수록 딜레이는 커지는 거니까
서비스가 커지거나 아니면
데이터가 많아지면은
말 그대로 검색 엔진이 이제
아예
분산으로 돌려야 될 것 같아요 사실은
근데 그 정도 스케일이 되면은
어차피 엘라스틱 설치 엔진
꼽아야 되거든요 그럼 그 순간에
아까 말한 아리랑 플러그인 꼽을 수 있어요
그러면
단데서 포팅할 방법이 이유가 없는 거야
그래서
당분간은 좀
어렵지 않을까 아니
하지 않을 것 같고 영원히
안 할 수도 있다는 생각을 해요
그래서 이제 그거 때문에 이제
저만의 그 이제
sql 긁어갖고 검색하는 그
거의 이제
좀 페이지 랭킹이라 그러나
그런 알고리즘을 만들어야 되고
뭐가 더 중요하고 뭐가 더 중요하고 이런 것들
루틴에서 보면은 그냥 부스팅이라고 하는
부분이죠 그런 걸 만들었고
그래도 모든 검색어를
칠때마다 3~4초가 걸리는 거 짜증이 나잖아요
그래서
그 인덱싱을
만드는 대신에 memcache를 많이 박았어요
그래서 이제
memcache를 박으면서
그러면 일단 이 검색어를 한번 검색을 했으면
모든 비디오에 대해선
이제 요 검색어에
그 스코어를 주는 거죠
요 검색어는 이 비디오는 100점 저건
32점 저건 150점 이런 식으로
그럼 그 스코어를 계산을 한 거를
이제 메모리에 박아두고
똑같은 검색어가 다시 들어오면은
그 결과만 보면은 나오는 거니까
그런 식으로 일단
이미 검색된 결과에 대한 memcache를
박긴 박았어요 그래서
만약에 포프TV 서치엔진 들어갔는데
검색을 했는데 갑자기 되게 빨리 나왔다
그건 누군가 검색해본 얘기에요
네 일단은
페이지에 오시는 분들 검색어가 비슷할 거라고 생각을 해서
일단 시작을 그렇게 했고
그리고 그 다음 단계로 이제
이거를 좀 더 빠르게 만들려고 한다면
그 검색어에 기반을 해서
그 실시간으로 그냥 인덱스를 만들어서
인덱스를 딴 데 보관해놓는 방법도 있죠
그리고 새로운 비디오가 올 때마다
이제 그 인덱스를 다시 업데이트하면 되는 거고
그래서 그거는
future plan으로 있어요 그래서
실시간 이제 검색어에 기반한
이제 인덱스 빌딩 시스템을 이제
생각을 하고 있고
뭐 페이지랭킹 시스템이죠 정확히 얘기하면은
그래서 그거는 뭐 추가될 수도
안 될 수도 있는 거 현재로는
뭐 검색어
놓고 3초 기다리는 거 나쁘지 않더라고요
일단은 그렇게 하고
그래서 거기까지가 제가 루신엔진
보고 뭐하고
참
다양한 걸 배웠어요 그렇게 해서
그리고
또 하나 이제
이 프로젝트를 하면서
비디오 길어지고 있지
여기서 빨리 맞춰야지 마지막으로
하나 얘기하고 싶었던 거는
제가 이제 인터페이스 떡칠하는 거나
디펜던시 인젝션
남용하는 거에 대해서 별로 안 좋게 봐요
근데 닷넷코어도
기본적으로 기본 서비스는
디펜던시 인젝션으로 다
지원이 되거든요
닷넷코어에서 그거를 안 쓰면은 되게 불편한
부분들이 많아요 물론
여러 가지들은 이제
다른 패턴으로 바꿔갖고 할 수도 있고
글로와이스도 실제 그러지만
DB 컨텍스트
하나만은 정말
디펜던시 인젝션을 제가 쓰거든요 안 쓰거든요
안 쓰기에는 걔네들이 넣어놓은
그 온갖
트랜지언트 스코프에 있는
라이프사이클 관리라던가 이게
제가 직접 만들어서 너무 싫었어요
근데 이 프로젝트 할 때는
그래 일단 내가
DI에 대해서
생각하는 바가 있지만
과연 이게
사람들이 말하는 것만큼 그렇게 훌륭한 건지
한번 또 기회를 한번 더 줘보자
예전에도 기회를 줘봤지만
그래서 모든 거를 그냥 닷넷코어에서
하라는 방식대로 다
디펜던시 인젝션으로 이제 실행을 해봤어요
그래서
모든 이제
뭐 스타트업 때
서비스 다 등록해두고
DI 하라는 대로 다 했는데
뭐
디버깅 할 때 이제
정확히 어떤 게 어떤 거를
호출하고 어떤 생성자가 호출되고
이런 부분에 문제는 있겠죠
근데 저는 DI 자체 인젝션을 다
이제 인터페이스가 아니라
컨크리트 클래스로 했기 때문에
그 문제는 좀 적었긴 했어요
근데 결과적으로 마지막에 하나
걸렸던 건 뭐였냐면은
아까 말했던
백그라운드 워커가 있잖아요
유튜브 데이터 API 불러오는
이 워커가
이 워커가 이제
실행을 한 다음에 그 다음에 어떤 자료들을
같은 걸 이제 DB에 넣어요
그럼 DB에 넣으면 DB 컨텍스트가 필요하거든요
근데 백그라운드 워커는
당연히 싱글 턴을 돌 수밖에 없어요
트랜지언트는 이제 한 리퀘스트가 있는 동안
도는 거지만 이건 그게 아니잖아요
그럼 글로벌 스코프에서 이제
싱글 턴을 도는 거고
그런데 닷넷 코어에서 기본적으로
이제 DB 컨텍스트는
트랜지언트 스코프에요
그래서 제가 그 백그라운드 워커에서
DB 컨텍스트 쓰려 그러면 에라가 나요
그럴 수가 없다고
왜냐면 라이프가, 그니까 오브젝트가
이미 죽어 있으니까
그래서 그 부분이 또
안되더라구요
그래서 이제 그것 좀 핵으로 일단 해결을 해놓긴 했는데
뭐
DI를 하면서 이제 컨크리트
이제 서비스만
이제 집어넣는다면은
뭐 처음
이제 시작하는 사람들한테 약간 러닝커브는 있겠지만
아주 나쁘진 않다고 생각을 하는데
결국 이제 DI가
라이프 사이클을 이제
접목시키면서 이렇게 만들고 어떤
그런 어썸션을 세우다 보니까
아주
자주는 아니지만 그래도 흔히 쓰는 이런
백그라운드 워커 같은 패턴에서 이제 발목이
잡히더라구요 그래서
아
이거 한 다음에 제가 글로와 있는 저희
디렉터 오브 엔지니어 친구한테
얘기를 했죠 야 이런 것 때문에 역시
또 아직도 DI가 좀 문제가 있더라
그니까 걔도 얼마 전에 자기들보다 똑같은 걸
느껴갖고 지금 자기도 좀
아직은 아니라고 생각을 하고 있다고
그래서 그거에 관해서
또 자료도 좀 찾아봤는데
궁금한 게 있으니까
그
DB 컨텍스트 매니지먼트를
닷넷 코어에서
팩토리 패턴 방식으로 했다면은
그 문제가 없어질 거다라는 얘기가
있고 생각해보니까 그것도 맞아요
그래서 어찌보면은
현재 있는 트랜지언트 방식 외에도
팩토리 방식을 지원할 수도 있다고 생각을 하는데
그러면
다시 한번 이제
DI를 더 많이 쓰는 걸
고민해볼 수도 있을 것 같긴 해요
음
이제
쓰지 않게 냅뒀으면은
그냥 안 쓸텐데 강요를 하는 상황에서
이게 안 좋다고
피해가느라고 여러가지 스택을
쌓는 게 여태까진 상관이 없었는데 특히
DB 이
컨텍스트 쪽에서 되게 힘들어가지고
그래서
그것도 이제 또 아마
고민을 하게 된 것 중에 하나에요 그냥
한번 더 기회 줘보고
아직은 아니다 이런 방식으로
...
...
...
...
그럼 오늘 비디오는 그 정도면은 될 것 같고
음
...
너무 길었다 포프TV 그냥
검색엔진을 많이 써주면 될 것 같아요
저한테 광고비가 더 들어오게
포프였습니다
